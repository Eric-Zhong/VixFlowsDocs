{
  "5": {
    "inputs": {
      "text": [
        "45",
        0
      ],
      "clip": [
        "50",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "6": {
    "inputs": {
      "conditioning": [
        "5",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "ConditioningZeroOut"
    }
  },
  "7": {
    "inputs": {
      "guidance": 10,
      "conditioning": [
        "5",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "FluxGuidance"
    }
  },
  "8": {
    "inputs": {
      "positive": [
        "7",
        0
      ],
      "negative": [
        "6",
        0
      ],
      "vae": [
        "48",
        2
      ],
      "pixels": [
        "17",
        0
      ]
    },
    "class_type": "InstructPixToPixConditioning",
    "_meta": {
      "title": "InstructPixToPixConditioning"
    }
  },
  "9": {
    "inputs": {
      "seed": 420760748224250,
      "steps": 12,
      "cfg": 1,
      "sampler_name": "dpmpp_2m",
      "scheduler": "beta",
      "denoise": 1,
      "model": [
        "32",
        0
      ],
      "positive": [
        "25",
        0
      ],
      "negative": [
        "8",
        1
      ],
      "latent_image": [
        "8",
        2
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "11": {
    "inputs": {
      "samples": [
        "9",
        0
      ],
      "vae": [
        "48",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "17": {
    "inputs": {
      "ckpt_name": "depth_anything_v2_vitl.pth",
      "resolution": 1024,
      "image": [
        "37",
        0
      ]
    },
    "class_type": "DepthAnythingV2Preprocessor",
    "_meta": {
      "title": "Depth Anything V2 - Relative"
    }
  },
  "25": {
    "inputs": {
      "downsampling_factor": 2,
      "downsampling_function": "area",
      "mode": "center crop (square)",
      "weight": 0.8,
      "autocrop_margin": 0.1,
      "conditioning": [
        "8",
        0
      ],
      "style_model": [
        "26",
        0
      ],
      "clip_vision": [
        "27",
        0
      ],
      "image": [
        "37",
        0
      ]
    },
    "class_type": "ReduxAdvanced",
    "_meta": {
      "title": "ReduxAdvanced"
    }
  },
  "26": {
    "inputs": {
      "style_model_name": "flux1-redux-dev.safetensors"
    },
    "class_type": "StyleModelLoader",
    "_meta": {
      "title": "Load Style Model"
    }
  },
  "27": {
    "inputs": {
      "clip_name": "sigclip_vision_patch14_384.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "32": {
    "inputs": {
      "weight": 0.9,
      "start_at": 0,
      "end_at": 1,
      "model": [
        "50",
        0
      ],
      "pulid_flux": [
        "33",
        0
      ],
      "eva_clip": [
        "34",
        0
      ],
      "face_analysis": [
        "35",
        0
      ],
      "image": [
        "37",
        0
      ]
    },
    "class_type": "ApplyPulidFlux",
    "_meta": {
      "title": "Apply PuLID Flux"
    }
  },
  "33": {
    "inputs": {
      "pulid_file": "pulid_flux_v0.9.1.safetensors"
    },
    "class_type": "PulidFluxModelLoader",
    "_meta": {
      "title": "Load PuLID Flux Model"
    }
  },
  "34": {
    "inputs": {},
    "class_type": "PulidFluxEvaClipLoader",
    "_meta": {
      "title": "Load Eva Clip (PuLID Flux)"
    }
  },
  "35": {
    "inputs": {
      "provider": "CPU"
    },
    "class_type": "PulidFluxInsightFaceLoader",
    "_meta": {
      "title": "Load InsightFace (PuLID Flux)"
    }
  },
  "36": {
    "inputs": {
      "filename_prefix": "pencil-sketch/img",
      "images": [
        "11",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "37": {
    "inputs": {
      "upscale_method": "lanczos",
      "megapixels": 1,
      "image": [
        "47",
        0
      ]
    },
    "class_type": "ImageScaleToTotalPixels",
    "_meta": {
      "title": "Scale Image to Total Pixels"
    }
  },
  "40": {
    "inputs": {
      "state": true,
      "display_name": "Use Gemini instead of Ollama",
      "optional": true,
      "advanced": true,
      "order": 95,
      "custom_id": "vision_provider",
      "hidden": false,
      "input_off_state": [
        "43",
        0
      ],
      "input_on_state": [
        "41",
        0
      ]
    },
    "class_type": "VixUiCheckboxLogic",
    "_meta": {
      "title": "VixUI-CheckboxLogic"
    }
  },
  "41": {
    "inputs": {
      "prompt": [
        "42",
        0
      ],
      "safety_settings": "BLOCK_NONE",
      "response_type": "text",
      "model": "gemini-1.5-flash-002",
      "api_key": "",
      "proxy": "",
      "system_instruction": "",
      "error_fallback_value": "",
      "image_1": [
        "37",
        0
      ]
    },
    "class_type": "Ask_Gemini",
    "_meta": {
      "title": "Ask Gemini"
    }
  },
  "42": {
    "inputs": {
      "text": "Generate a structured caption for the given image.\n\n    Detect and describe the position of all visible subjects or objects relative to the frame.\n    Provide description of the subjects, including physical appearance, attire, accessories, and expressions.\n    Detect and transcribe visible text, specifying its position and font style.\n    Conclude with the overall mood or atmosphere.\n\nDo not describe or pay attention to the background.\n\nThe description must be accurate, comprehensive, logically organized, and contain no more than 46 words.\n"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "43": {
    "inputs": {
      "query": [
        "42",
        0
      ],
      "debug": "enable",
      "url": "http://127.0.0.1:11434",
      "model": "llava:7b-v1.6-vicuna-q8_0",
      "keep_alive": 0,
      "format": "text",
      "seed": 2130378911,
      "images": [
        "37",
        0
      ]
    },
    "class_type": "OllamaVision",
    "_meta": {
      "title": "Ollama Vision"
    }
  },
  "44": {
    "inputs": {
      "text": "pencil sketch, minimalist, impressionism, negative space"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Male"
    }
  },
  "45": {
    "inputs": {
      "delimiter": ", ",
      "clean_whitespace": "true",
      "text_a": [
        "44",
        0
      ],
      "text_b": [
        "40",
        0
      ]
    },
    "class_type": "Text Concatenate",
    "_meta": {
      "title": "Text Concatenate"
    }
  },
  "46": {
    "inputs": {
      "name": "pencil_sketch_portrait",
      "display_name": "Pencil Sketch Portrait",
      "description": "Sketch portrait using pencil",
      "author": "Datou",
      "homepage": "https://openart.ai/workflows/datou/all-your-life-ver-12/KdNc8OnmzTieGRBkGHks",
      "documentation": "",
      "license": "",
      "tags": "[\"general\", \"portrait\", \"sketch\"]",
      "version": "1.0.0",
      "requires": "[\"Ollama:llava:7b-v1.6-vicuna-q8_0\"]",
      "is_seed_supported": true,
      "is_count_supported": true,
      "is_translations_supported": false,
      "is_macos_supported": true,
      "required_memory_gb": 12
    },
    "class_type": "VixUiWorkflowMetadata",
    "_meta": {
      "title": "VixUI-WorkflowMetadata"
    }
  },
  "47": {
    "inputs": {
      "image": "",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "input;Person's face;order=1;custom_id=person_face"
    }
  },
  "48": {
    "inputs": {
      "ckpt_name": "flux\\flux1-dev-fp8.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "50": {
    "inputs": {
      "lora_01": "pencilbox.safetensors",
      "strength_01": 1,
      "lora_02": "flux1-depth-dev-lora.safetensors",
      "strength_02": 0.8,
      "lora_03": "FLUX.1-Turbo-Alpha.safetensors",
      "strength_03": 1,
      "lora_04": "None",
      "strength_04": 1,
      "model": [
        "48",
        0
      ],
      "clip": [
        "48",
        1
      ]
    },
    "class_type": "Lora Loader Stack (rgthree)",
    "_meta": {
      "title": "Lora Loader Stack (rgthree)"
    }
  }
}
